{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasamanAkhoundzadeh/OSLab-Fall2022/blob/main/Detecting_IOT_BotNet_Atacks_On_N_BaIoT_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports And Initializations**"
      ],
      "metadata": {
        "id": "shoiAWJ-LehY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piT-xurhYHAa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import recall_score, f1_score\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import confusion_matrix, recall_score, classification_report, f1_score\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "from sklearn import svm"
      ],
      "metadata": {
        "id": "9CyGEtOzYYCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os"
      ],
      "metadata": {
        "id": "_b6pcIQ-OxGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "dataPath=\"/content/drive/MyDrive/University/projects/N-BaIoT/Colab Notebooks/Dataset\"\n",
        "#dataPath=\"/content/drive/MyDrive/Colab Notebooks/Dataset\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ_H_suQLdq-",
        "outputId": "996b061b-2dcd-4e11-d068-845fb114dcdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/University/projects/N-BaIoT/Colab Notebooks/Dataset\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEdV3UKQO-du",
        "outputId": "6af09ceb-b7a7-4ac3-93b2-0ff45ccf837a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.benign.csv\t      4.gafgyt.tcp.csv\t    7.gafgyt.scan.csv\n",
            "1.gafgyt.combo.csv    4.gafgyt.udp.csv\t    7.gafgyt.tcp.csv\n",
            "1.gafgyt.junk.csv     4.mirai.ack.csv\t    7.gafgyt.udp.csv\n",
            "1.gafgyt.scan.csv     4.mirai.scan.csv\t    8.benign.csv\n",
            "1.gafgyt.tcp.csv      4.mirai.syn.csv\t    8.gafgyt.combo.csv\n",
            "1.gafgyt.udp.csv      4.mirai.udp.csv\t    8.gafgyt.junk.csv\n",
            "1.mirai.ack.csv       4.mirai.udpplain.csv  8.gafgyt.scan.csv\n",
            "1.mirai.scan.csv      5.benign.csv\t    8.gafgyt.tcp.csv\n",
            "1.mirai.syn.csv       5.gafgyt.combo.csv    8.gafgyt.udp.csv\n",
            "1.mirai.udp.csv       5.gafgyt.junk.csv     8.mirai.ack.csv\n",
            "1.mirai.udpplain.csv  5.gafgyt.scan.csv     8.mirai.scan.csv\n",
            "2.benign.csv\t      5.gafgyt.tcp.csv\t    8.mirai.syn.csv\n",
            "2.gafgyt.combo.csv    5.gafgyt.udp.csv\t    8.mirai.udp.csv\n",
            "2.gafgyt.junk.csv     5.mirai.ack.csv\t    8.mirai.udpplain.csv\n",
            "2.gafgyt.scan.csv     5.mirai.scan.csv\t    9.benign.csv\n",
            "2.gafgyt.tcp.csv      5.mirai.syn.csv\t    9.gafgyt.combo.csv\n",
            "2.gafgyt.udp.csv      5.mirai.udp.csv\t    9.gafgyt.junk.csv\n",
            "2.mirai.ack.csv       5.mirai.udpplain.csv  9.gafgyt.scan.csv\n",
            "2.mirai.scan.csv      6.benign.csv\t    9.gafgyt.tcp.csv\n",
            "2.mirai.syn.csv       6.gafgyt.combo.csv    9.gafgyt.udp.csv\n",
            "2.mirai.udp.csv       6.gafgyt.junk.csv     9.mirai.ack.csv\n",
            "2.mirai.udpplain.csv  6.gafgyt.scan.csv     9.mirai.scan.csv\n",
            "3.benign.csv\t      6.gafgyt.tcp.csv\t    9.mirai.syn.csv\n",
            "3.gafgyt.combo.csv    6.gafgyt.udp.csv\t    9.mirai.udp.csv\n",
            "3.gafgyt.junk.csv     6.mirai.ack.csv\t    9.mirai.udpplain.csv\n",
            "3.gafgyt.scan.csv     6.mirai.scan.csv\t    data_summary.csv\n",
            "3.gafgyt.tcp.csv      6.mirai.syn.csv\t    desktop.ini\n",
            "3.gafgyt.udp.csv      6.mirai.udp.csv\t    device_info.csv\n",
            "4.benign.csv\t      6.mirai.udpplain.csv  features.csv\n",
            "4.gafgyt.combo.csv    7.benign.csv\t    README.md\n",
            "4.gafgyt.junk.csv     7.gafgyt.combo.csv\n",
            "4.gafgyt.scan.csv     7.gafgyt.junk.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qrLgtKlJtBq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainFunction(x_train_data, y_train_data):\n",
        "    print(\"\\n--- trainFunction is running ---\\n\")\n",
        "\n",
        "    fold = KFold(len(x_train_data), 5, shuffle=False)\n",
        "\n",
        "    c_params = [0.01, 0.1, 1, 10, 100]\n",
        "    result_tables = pd.DataFrame(columns=['C_parameter', 'Mean recall score'])\n",
        "    result_tables['C_parameter'] = c_params\n",
        "\n",
        "    j = 0\n",
        "    for c_param in c_params:\n",
        "        print('-------------------------------------------')\n",
        "        print('C_parameters are equal to:', c_param)\n",
        "        print('-------------------------------------------\\n')\n",
        "\n",
        "        recall_list = []\n",
        "        for iteration, indices in enumerate(fold, start=1):\n",
        "            nb = GaussianNB()\n",
        "            nb.fit(x_train_data.iloc[indices[0], :], y_train_data.iloc[indices[0], :].values.ravel())\n",
        "\n",
        "            y_undersample_pred = nb.predict(x_train_data.iloc[indices[1], :].values)\n",
        "            recall = recall_score(y_train_data.iloc[indices[1], :].values, y_undersample_pred)\n",
        "\n",
        "            f1_score_value = f1_score(y_train_data.iloc[indices[1], :].values, y_undersample_pred)\n",
        "            recall_list.append(recall)\n",
        "            print(\"Iteration\", iteration, \"Recall:\", recall, \"f1_score_value:\", f1_score_value)\n",
        "\n",
        "        print('-------------------------------------------')\n",
        "        print('Average Recall:', np.mean(recall_list))\n",
        "        print('-------------------------------------------')\n",
        "        result_tables.loc[j, 'Mean recall score'] = np.mean(recall_list)\n",
        "        j = j + 1\n",
        "\n",
        "    result_tables['Mean recall score'] = result_tables['Mean recall score'].astype('float64')\n",
        "    best_c_param = result_tables.loc[result_tables['Mean recall score'].idxmax(), 'C_parameter']\n",
        "\n",
        "    print('---------------------------------------------')\n",
        "    print('Best C_parameter rate for the model equals to:', best_c_param)\n",
        "    print('---------------------------------------------')\n",
        "\n",
        "    return best_c_param\n",
        "\n"
      ],
      "metadata": {
        "id": "B3kNQgqcYkVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def showData(data):\n",
        "    #print(data.shape)\n",
        "    print(data.head())"
      ],
      "metadata": {
        "id": "xkac4n6vYo9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9# Using the virtual server to run the pprogram with all datasets.\n",
        "\n",
        "print(\"The maximum value could be 9!\\n\")\n",
        "first = int(input(\"First number for the loop to set the dataset range: \"))\n",
        "last = int(input(\"Last number for the loop to set the dataset range: \"))\n",
        "\n",
        "# Defining all lists before Going to prepare them and use them in the LOOP.\n",
        "\n",
        "bng = []\n",
        "m_udp = []\n",
        "m_ack = []\n",
        "m_scan = []\n",
        "m_syn = []\n",
        "m_udpplain = []\n",
        "g_combo = []\n",
        "g_junk = []\n",
        "g_scan = []\n",
        "g_tcp = []\n",
        "g_udp = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBklTe8sZSzA",
        "outputId": "658973be-175f-4d0f-ef72-5321de2861ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The maximum value could be 9!\n",
            "\n",
            "First number for the loop to set the dataset range: 1\n",
            "Last number for the loop to set the dataset range: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataPrepare():\n",
        "\n",
        "    print(\"\\n--- dataPrepare is running ---\\n\")\n",
        "    j = 0\n",
        "    result = pd.DataFrame()\n",
        "    for i in range (first, last):\n",
        "\n",
        "        # benign dataset\n",
        "        csv_bng = pd.read_csv(f\"{dataPath}/{i}.benign.csv\")\n",
        "        csv_bng['Class'] = 0\n",
        "        #bng.insert(i, csv_bng)\n",
        "\n",
        "\n",
        "        # gafgyt dataset combo\n",
        "        csv_g_combo = pd.read_csv(f\"{dataPath}/{i}.gafgyt.combo.csv\")\n",
        "        csv_g_combo['Class'] = 1\n",
        "        #g_combo.insert(i, csv_g_combo)\n",
        "        # gafgyt dataset junk\n",
        "        csv_g_junk = pd.read_csv(f\"{dataPath}/{i}.gafgyt.junk.csv\")\n",
        "        csv_g_junk['Class'] = 1\n",
        "        #g_junk.insert(i, csv_g_junk)\n",
        "\n",
        "        # gafgyt dataset scan\n",
        "        csv_g_scan = pd.read_csv(f\"{dataPath}/{i}.gafgyt.scan.csv\")\n",
        "        csv_g_scan['Class'] = 1\n",
        "        #g_scan.insert(i, csv_g_scan)\n",
        "\n",
        "        # gafgyt dataset tcp\n",
        "        csv_g_tcp = pd.read_csv(f\"{dataPath}/{i}.gafgyt.tcp.csv\")\n",
        "        csv_g_tcp['Class'] = 1\n",
        "        #g_tcp.insert(i, csv_g_tcp)\n",
        "\n",
        "        # gafgyt dataset udp\n",
        "        csv_g_udp = pd.read_csv(f\"{dataPath}/{i}.gafgyt.udp.csv\")\n",
        "        csv_g_udp['Class'] = 1\n",
        "        #g_udp.insert(i, csv_g_udp)\n",
        "\n",
        "        #frames = [bng, m_udp, m_ack, m_scan, m_syn, m_udpplain ,g_combo, g_junk, g_scan, g_tcp, g_udp]\n",
        "        result = pd.concat([result, csv_bng, csv_g_combo, csv_g_junk, csv_g_scan, csv_g_tcp, csv_g_udp], ignore_index=True)\n",
        "\n",
        "         # The reason we used while loop here is that we DON'T have any 'mirai' type dataset with number of \"3\".\n",
        "        if i != 3 and i!=7:\n",
        "            # mirai dataset udp\n",
        "            csv_m_udp = pd.read_csv(f\"{dataPath}/{i}.mirai.udp.csv\")\n",
        "            csv_m_udp['Class'] = 1\n",
        "            #m_udp.insert(i, csv_m_udp)\n",
        "\n",
        "            # mirai dataset ack\n",
        "            csv_m_ack = pd.read_csv(f\"{dataPath}/{i}.mirai.ack.csv\")\n",
        "            csv_m_ack['Class'] = 1\n",
        "            #m_ack.insert(i, csv_m_ack)\n",
        "\n",
        "            # mirai dataset ack\n",
        "            csv_m_scan = pd.read_csv(f\"{dataPath}/{i}.mirai.scan.csv\")\n",
        "            csv_m_scan['Class'] = 1\n",
        "            #m_scan.insert(i, csv_m_scan)\n",
        "\n",
        "            # mirai dataset syn\n",
        "            csv_m_syn = pd.read_csv(f\"{dataPath}/{i}.mirai.syn.csv\")\n",
        "            csv_m_syn['Class'] = 1\n",
        "            #m_syn.insert(i, csv_m_syn)\n",
        "\n",
        "            # mirai dataset udpplain\n",
        "            csv_m_udpplain = pd.read_csv(f\"{dataPath}/{i}.mirai.udpplain.csv\")\n",
        "            csv_m_udpplain['Class'] = 1\n",
        "            #m_udpplain.insert(i, csv_m_udpplain)\n",
        "            # While Loop ends here\n",
        "            result = pd.concat([result, csv_m_udp, csv_m_ack, csv_m_scan, csv_g_tcp, csv_g_udp], ignore_index=True)\n",
        "\n",
        "    # concatinate all results in one list\n",
        "    #result = pd.concat([bng, g_combo, g_junk, g_scan, g_tcp, g_udp], ignore_index=True)\n",
        "\n",
        "    showData(result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "Ui3vxy-AZWw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataPreprocessing(data):\n",
        "\n",
        "    print(\"\\n--- dataPreprocessing Function is running ---\\n\")\n",
        "\n",
        "    count_class = pd.value_counts(data['Class'],sort=True).sort_index()\n",
        "    print(count_class)\n",
        "\n",
        "    print('------')\n",
        "\n",
        "    X = data.iloc[:, data.columns != 'Class']\n",
        "    y = data.iloc[:, data.columns == 'Class']\n",
        "\n",
        "    positive_sample_count = len(data[data.Class == 1])\n",
        "    print(\"Positive sample counts equals to：\", positive_sample_count)\n",
        "\n",
        "    negative_sample_index = np.array(data[data.Class == 0].index)\n",
        "    print(\"[First Five] Negative sample counts equals to：\", negative_sample_index[:5])\n",
        "\n",
        "    positive_sample_index = data[data.Class == 1].index\n",
        "\n",
        "    # generate a random sample from the given 1D array with the below code\n",
        "    # numpy.random.choice(a, size=None, replace=True, p=None)\n",
        "\n",
        "    random_positive_sample_index = np.random.choice(positive_sample_index, int(1*len(data[data.Class == 0])), replace=False)\n",
        "\n",
        "    # Adjust the number of samples when detecting anomalies\n",
        "    # negative_sample_index = np.random.choice(positive_sample_index, len(data[data.Class == 0]), replace=False)\n",
        "\n",
        "    print(\"[First Five] Positive sample counts equals to：\", random_positive_sample_index[:5])\n",
        "\n",
        "    under_sample_index = np.concatenate([random_positive_sample_index, negative_sample_index])\n",
        "    under_sample_data = data.iloc[under_sample_index, :]\n",
        "\n",
        "    X_under_sample = under_sample_data.iloc[:, under_sample_data.columns != 'Class']\n",
        "    y_under_sample = under_sample_data.iloc[:, under_sample_data.columns == 'Class']\n",
        "\n",
        "    print('[After Downsampling] The proportion of +POSITIVE+ samples：', len(under_sample_data[under_sample_data.Class == 1]) / len(under_sample_data))\n",
        "    print('[After Downsampling] The proportion of -NEGATIVE- samples：', len(under_sample_data[under_sample_data.Class == 0]) / len(under_sample_data))\n",
        "    print('[After Downsampling] The Length of Under Sample Data: ', len(under_sample_data))\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "    X_train_under_sample, X_test_under_sample, y_train_under_sample, y_test_under_sample = train_test_split(X_under_sample, y_under_sample, test_size=0.3, random_state=0)\n",
        "\n",
        "\n",
        "    print('Number of samples in the \"Training\" set：', len(X_train_under_sample))\n",
        "    print('Number of samples in the \"Test\" set：', len(X_test_under_sample))\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, X_train_under_sample, X_test_under_sample, y_train_under_sample, y_test_under_sample\n"
      ],
      "metadata": {
        "id": "5lmmXlZ7ZmPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(confusion_matrix, classes):\n",
        "    print(\"\\n--- plot_confusion_matrix is running ---\\n\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=0)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    thresh = confusion_matrix.max() / 2.\n",
        "    range_0 = range(confusion_matrix.shape[0])\n",
        "    range_1 = range(confusion_matrix.shape[1])\n",
        "\n",
        "    for i, j in itertools.product(range_0, range_1):\n",
        "        plt.text(j, i, confusion_matrix[i, j], horizontalalignment=\"center\",\n",
        "                 color=\"white\" if confusion_matrix[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "    precision = confusion_matrix[1, 1] / (confusion_matrix[1, 1] + confusion_matrix[0, 1])\n",
        "    recall = confusion_matrix[1, 1] / (confusion_matrix[1, 1] + confusion_matrix[1, 0])\n",
        "    accuracy = (confusion_matrix[0, 0] + confusion_matrix[1, 1]) / (\n",
        "                confusion_matrix[0, 0] + confusion_matrix[0, 1] + confusion_matrix[1, 1] + confusion_matrix[1, 0])\n",
        "\n",
        "    print('Precision rate:', precision)\n",
        "    print('Recall rate:', recall)\n",
        "    print('Accuracy:', accuracy)\n",
        "    print('*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')\n",
        "\n",
        "result = dataPrepare()\n",
        "X_train, X_test, y_train, y_test, X_train_under_sample, X_test_under_sample, y_train_under_sample, y_test_under_sample = dataPreprocessing(result)\n",
        "\n",
        "# Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train_under_sample, y_train_under_sample.values.ravel())\n",
        "\n",
        "y_undersample_pred = nb.predict(X_test_under_sample)\n",
        "\n",
        "# Set threshold for prediction\n",
        "threshold = 0.5\n",
        "y_undersample_pred_binary = [1 if pred >= threshold else 0 for pred in y_undersample_pred]\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test_under_sample, y_undersample_pred_binary)\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "class_names = [0, 1]\n",
        "\n",
        "plot_confusion_matrix(conf_matrix, classes=class_names)\n",
        "\n",
        "# LightGBM\n",
        "params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9\n",
        "}\n",
        "\n",
        "train_data = lgb.Dataset(X_train_under_sample, label=y_train_under_sample)\n",
        "gbm = lgb.train(params, train_data)\n",
        "\n",
        "y_undersample_pred = gbm.predict(X_test_under_sample)\n",
        "\n",
        "# Set threshold for prediction\n",
        "threshold = 0.5\n",
        "y_undersample_pred_binary = [1 if pred >= threshold else 0 for pred in y_undersample_pred]\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test_under_sample, y_undersample_pred_binary)\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "class_names = [0, 1]\n",
        "\n",
        "plot_confusion_matrix(conf_matrix, classes=class_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B9kGdLFaD8Z",
        "outputId": "e8a4f793-6573-4e15-965d-2cb8111b4290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- dataPrepare is running ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kt55fOin1oq-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}